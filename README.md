# Speech_Emotion_Recognition
Emotion recognition is an important but challenging 
subject in recent years. This project explores the speech emotion 
system (SER) with several models (MLP, SVM, CNN) to recognize 
different emotions correctly. The dataset used in this project is 
The Ryerson Audio-Visual Database of Emotional Speech and 
Song (RAVDESS). Some features such as Mel-frequency Cepstral 
Coefficients (MFCCs), Mel-scaled spectrogram and Chromogram 
are extracted to catch time-frequency domain information then be 
fed into models, data augmentation method is also implemented to 
improve the training performance. The SER system shows the CNN model acquire the accuracy of 80% for 
total 10 classes
